# Memory-efficient configuration optimized for NVIDIA GeForce GTX 1650 (3.81 GB VRAM)
# This configuration reduces memory usage while maintaining model functionality

# Model configuration - reduced sizes for memory efficiency
model_name: "bert-base-uncased"
num_classes: 2
state_size: 128  # Reduced from 256
num_emotions: 4  # Reduced from 8
reflect_layers: 1  # Reduced from 2
memory_size: 256  # Reduced from 512
num_decisions: 2  # Reduced from 4
cnn_filters: 64  # Reduced from 128
cnn_kernels: [3, 4]  # Reduced from [3, 4, 5]
compile_model: false  # Disabled to save memory and avoid Python 3.13 issues
compile_mode: "default"
suppress_compile_errors: true
# Architectural options - tuned for low VRAM
use_eps: true
eps_hidden_size: 64
eps_num_heads: 2
use_adapters: true
adapter_reduction: 16
use_residual_reflection: true

# Training configuration - optimized for GTX 1650
output_dir: "./out"
evaluation_strategy: "epoch"
save_strategy: "epoch"
per_device_train_batch_size: 4  # Reduced from 16 to fit in 3.81 GB VRAM
per_device_eval_batch_size: 8   # Slightly larger for eval since no gradients
num_train_epochs: 10  # Reduced from 100 for faster testing
learning_rate: 0.00005  # 5e-5, slightly higher due to smaller batch size
weight_decay: 0.01
logging_steps: 25  # More frequent logging due to smaller batches
save_total_limit: 1  # Save only the best model to save disk space
load_best_model_at_end: true
metric_for_best_model: "matthews_correlation"
greater_is_better: true

# Mixed precision settings - essential for memory efficiency
fp16: true
fp16_opt_level: "O1"

# Gradient accumulation - increase to maintain effective batch size
gradient_accumulation_steps: 4  # Effective batch size = 4 * 4 = 16

# Learning rate scheduler
lr_scheduler_type: "linear"
warmup_ratio: 0.1

# Early stopping - more aggressive to save time
early_stopping_patience: 2  # Reduced from 3
early_stopping_threshold: 0.001  # Slightly higher threshold
