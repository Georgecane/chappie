# Model configuration
model_name: "bert-base-uncased"
num_classes: 2
state_size: 256
num_emotions: 8
reflect_layers: 2
memory_size: 512
num_decisions: 4
cnn_filters: 128
cnn_kernels: [3, 4, 5]
compile_model: true
compile_mode: "max-autotune"
suppress_compile_errors: true  # Fall back to eager mode if compilation fails
# Architectural options
use_eps: true
eps_hidden_size: 128
eps_num_heads: 4
use_adapters: true
adapter_reduction: 16
use_residual_reflection: true

# Training configuration
output_dir: "./out"
evaluation_strategy: "epoch"
save_strategy: "epoch"
per_device_train_batch_size: 16
per_device_eval_batch_size: 16
num_train_epochs: 100
learning_rate: 0.00002  # 2e-5 as a decimal to avoid parsing issues
weight_decay: 0.01
logging_steps: 50
save_total_limit: 2
load_best_model_at_end: true
metric_for_best_model: "matthews_correlation"
greater_is_better: true

# Mixed precision settings
fp16: true
fp16_opt_level: "O1"

# Gradient accumulation
gradient_accumulation_steps: 2

# Learning rate scheduler
lr_scheduler_type: "linear"
warmup_ratio: 0.1

# Early stopping
early_stopping_patience: 3
early_stopping_threshold: 0.0
